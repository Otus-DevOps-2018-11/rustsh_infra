# rustsh_infra
rustsh Infra repository

## Домашнее задание № 5 (знакомство с облачной инфраструктурой)

### Подключение к удалённой машине в одну строку
```
ssh -At rustsh@35.210.23.56 ssh 10.132.0.3
```
где ключ `-t` означает подключение псевдотерминала.

### Дополнительное задание: подключение из консоли при помощи команды вида `ssh someinternalhost` 
Для подключения при помощи команды вида `ssh someinternalhost` из локальной консоли рабочего устройства, чтобы подключение выполнялось по алиасу `someinternalhost`, необходимо создать файл `~/.ssh.config` и добавить туда следующий текст:
```
Host bastion
	Hostname 35.210.23.56
	User rustsh
	
Host someinternalhost
	ProxyCommand ssh bastion -W 10.132.0.3:22
	User rustsh
```
где ключ `-W` означает создание туннеля.

### Данные для подключения
bastion_IP = 35.210.23.56  
someinternalhost_IP = 10.132.0.3

## Домашнее задание № 6 (деплой тестового приложения)

### Данные для подключения:
testapp_IP = 35.204.17.203  
testapp_port = 9292

### Выполнение стартового скрипта
Для запуска скрипта `startup.sh` при создании ВМ необходимо в команду gcloud добавить ключ `--metadata-from-file startup-script=./startup.sh`
Сама команда примет такой вид:
```
gcloud compute instances create reddit-app\
  --boot-disk-size=10GB \
  --image-family ubuntu-1604-lts \
  --image-project=ubuntu-os-cloud \
  --machine-type=g1-small \
  --tags puma-server \
  --restart-on-failure \
  --metadata-from-file startup-script=./startup.sh
```
В результате применения данной команды мы получим инстанс с уже запущенным приложением.

### Добавление правила файервола
Для создания правила файервола из консоли gcloud необходимо выполнить следующую команду:
```
gcloud compute firewall-rules create default-puma-server \
  --allow tcp:9292 \
  --target-tags=puma-server
```

## Домашнее задание № 7 (Сборка образов VM  при помощи Packer)

Что сделано:
1. Установлен packer.
2. Созданы Application Default Credentials (ADC).
3. Создан packer-шаблон ubuntu16.json, содержащий описание образа VM, который мы хотим создать, в нём указаны builders для характеристик машины, а также provisioners для установки Ruby и MongoDB при помощи скриптов. Сами скрипты nstall_ruby.sh и install_mongodb.sh добавлены в папку packer/scripts.
4. При помощи packer создан образ в GCP, из него развёрнута виртуальная машина, на которой установлено и запущено приложение reddit.
5. Создан файл variables.json, в котором заданы значения переменных для шаблона packer.
6. В шаблоне заданы дополнительные опции builder для GCP.

## Домашнее задание № 8 (Практика IaC  с использованием Terraform)

Что сделано:
1. Установлен terraform.
2. Удалены ключи пользователя из метаданных проекта.
3. Обновлён .gitignore (включены вспомогательные файлы и файл с определением переменных).
4. Создан файл main.tf, в нём заданы провайдер, ресурсы (VM и правило файервола), провижинеры.
5. При помощи команды `terraform init` загружен провайдер.
6. Созданы файлы output.tf для выходных переменных, variables.tf для входных переменных, terraform.tfvars для определения переменных.
7. Командой `terraform apply` созданы все описанные ресурсы.
8. После проверки работы созданного приложения ресурсы удалены командой `terraform destroy`.

## Домашнее задание № 9 (Terraform: ресурсы, модули, окружения и работа в команде)

Что сделано:
1. При помощи команды `terraform import` существующее правило файервола импортировано в state-файл.
2. Для инстанса с приложением задан IP в виде внешнего ресурса, внутри конфигурации ресурса VM создана ссылка на атрибуты ресурса, который этот IP создает (таким образом, создана неявная зависимость).
3. Кофигурация была разбита на несколько файлов, после чего для разных ресурсов созданы модули, которые были параметризированны.
4. Созданы конфигурации для двух окружений (stage и prod).
5. Описана конфигурация для создания бакетов в сервисе Storage при помощи внешнего модуля.

## Домашнее задание № 10 (Управление конфигурацией. Основные DevOps инструменты. Знакомство с Ansible)

Что сделано:
1. В WSL поднята ОС Linux, на ней установлен Ansible 2.7.
2. Созданы файлы inventory с описанием хостов, ansible.cfg с параметрами.
3. Опрообовано выполнение команд Ansible из командной строки.
4. Создан и запущен плейбук.

## Домашнее задание № 11 (Деплой и управление конфигурацией с Ansible)

Что сделано:
1. Создан один плейбук с одним сценарием для конфигурации хостов и деплоя приложений (разделение тасков при помощи тегов, указание хостов в командной строке), шаблоны для конфигурации приложений, заданы обработчики, исследована возможность передавать файлы на удалённый хост.
2. Создан плейбук с несколькими сценариями — теперь хосты и теги задаются на уровне сценариев.
3. Сценарии вынесены в отдельные плейбуки, а значит, необходимость в тегах отпала. Создан главный плейбук, который включает в себя все остальные.
4. Созданы плейбуки для провижининга в Packer, с их помощью созданы новые образы для инстансов приложения и БД.
5. Из новых образов при помощи Terraform поднята инфраструктура, а при помощи Ansible развёрнуты и настроены приложение и БД.

## Домашнее задание № 12 (Ansible: работа с ролями и окружениями)

Что сделано:
1. При помощи команды `ansible-galaxy init` созданы структуры ролей `app` и `db` для конфигурирования серверов приложения и БД соответственно.
2. В эти роли из общей папки перенесены таски, обработчики, шаблоны и файлы.
3. В старых плейбуках вместо всего этого оставлены только вызовы ролей.
4. Описаны `stage` и `prod` окружения, для каждого созданы файл с перечислением хостов (inventory) и файлы с переменными групп хостов для изменения настроек конфигурации в зависимости от окружения. Из самих плейбуков переменные удалены.
5. Организовано расположение файлов в директории ansible.
6. Загружена в соответствии с файлом зависимостей, установлена и настроена роль из репозитория Ansible Galaxy, устанавливающая реверс-прокси. Её вызов добавлен в плейбук.
7. В модуле с созданием правила файервола для сервера приложения в параметрах задано открытие 80-го порта.
8. При помощи `ansible-vault` зашифрованы файлы с секретами, файл с мастер-ключом от него добавлен в .gitignore и ansible.cfg.
9. Используя зашифрованные данные, посредством Ansible на удалённые хосты добавлены новые пользователи.

## Домашнее задание № 13 (Разработка и тестирование Ansible ролей и плейбуков)

Что сделано:
1. В файле Vagrantfile описана локальная инфраструктура.
2. Настроена совместимость между Vagrant внутри WSL и VirtualBox в Windows:
    - в WSL и Windows установлена одинаковая версия Vagrant (2.0.2);
    - в Windows путь до VirtualBox добавлен в PATH;
    - в WSL заданы переменные окружения:
        ```bash
        $ export VAGRANT_WSL_ENABLE_WINDOWS_ACCESS="1"
        $ export PATH="$PATH:/mnt/c/Program Files/Oracle/VirtualBox"
        $ export VAGRANT_WSL_WINDOWS_ACCESS_USER_HOME_PATH="/mnt/c/ansible"
        ```
    - в Vagrantfile в область настроек виртуальных машин добавлена строчка `v.customize [ "modifyvm", :id, "--uartmode1", "disconnected" ]` (без неё всё падает).
3. Командой `vagrant up` созданы виртуальные машины.
4. В Vagrantfile добавлен провижинер для dbserver.
5. Чтобы в WSL можно было настраивать права командой `chmod`, диск C: перемонтирован с метаданными:
    ```bash
    $ sudo umount /mnt/c
    $ sudo mount -t drvfs C: /mnt/c -o metadata
    ```
6. Чтобы Ansible выполнил плейбуки, настроены права для папки ansible и приватных ключей, созданных Vagrant'ом (выставлены более строгие).
7. В плейбуки добавлена установка Python, переработана роль db.
8. Аналогичные настройки сделаны для appserver.
9. Параметризировано имя пользователя на удалённой машине.
10. В WSL установлены Molecule и Testinfra.
11. Командой `molecule init` в роли db создана заготовка для тестов, в файл test_default.py добавлены сами тесты.
12. Чтобы Molecule из WSL заработал с VirtualBox в Windows, в файл molecule.yml в раздел platforms добавлен параметр provider_raw_config_args со значением `"customize [ 'modifyvm', :id, '--uartmode1', 'disconnected' ]"`.
13. Командой `molecule create` создана тестовая машина, к ней командой `molecule converge` применена роль db и командой `molecule verify` запущены тесты.
14. Добавлен тест для проверки того, что БД слушает порт 27017:
    ```python
    def test_mongo_listen_port(host):
        assert host.socket("tcp://0.0.0.0:27017").is_listening
    ```
15. В плейбуках packer_db.yml и packer_app.yml таски заменены на роли db и app, в шаблонах Packer'а app.json и db.json добавлена информация о тегах и пути до ролей.
